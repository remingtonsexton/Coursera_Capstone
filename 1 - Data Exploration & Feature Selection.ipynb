{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration & Feature Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IBM Data Science Professional Certificate - Capstone Project**\n",
    "<br><br/>\n",
    "**Remington Oliver Sexton, Ph.D.**\n",
    "<br><br/>\n",
    "In this notebook we perform some exploratory data analysis to determine which features of our dataset will be good predictors of our target variables for our machine learning model. \n",
    "<br> <br/>\n",
    "The data we are using for this project is compiled data from UK traffic accidents and vehicle information from the years 2005 through 2017, which can be accessed via [Kaggle](https://www.kaggle.com/tsiaras/uk-road-safety-accidents-and-vehicles?select=Accident_Information.csv).  The goal of this project is to develop a successful model that can predict the accident severity as a function of various predictors, such as road conditions and vehicle information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import folium # for mapping\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/accidents_all.csv')#,low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a list of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = df.columns.tolist()\n",
    "dtypes = df.dtypes.tolist()\n",
    "# Print these into two columns\n",
    "print('{0:<50}{1:<30}'.format('attribute', 'dtype'))\n",
    "print('----------------------------------------------------------------')\n",
    "for i in range(len(cols)):\n",
    "    print('{0:<50}{1:<30}'.format(cols[i], str(dtypes[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Unnecessary Columns\n",
    "\n",
    "Some of these columns do not contribute to *predicting* causes of accidents, but are descriptors of each incident or vehicle.  We drop the following columns:\n",
    "\n",
    "| Attribute                                    | Reason                                       |\n",
    "| -------------------------------------------- | -------------------------------------------- |\n",
    "| Unnamed: 0                                   | N/A                                          |\n",
    "| Did_Police_Officer_Attend_Scene_of_Accident  | N/A                                          |\n",
    "| Local_Authority_(Highway)                    | *Local_Authority_(District)* for mapping.    |\n",
    "| Location_Easting_OSGR                        | district is used for location mapping.       |\n",
    "| Location_Northing_OSGR                       | district is used for location mapping.       |\n",
    "| LSOA_of_Accident_Location                    | district is used for location mapping.       |\n",
    "| Police_Force                                 | N/A                                          |\n",
    "| InScotland                                   | N/A                                          |\n",
    "| Vehicle_Reference                            | N/A                                          |\n",
    "| model                                        | *make* used instead.                         |\n",
    "| Year_x                                       | *date* and *age_of_vehicle* used instead.    |\n",
    "| Year_y                                       | *date* and *age_of_vehicle* used instead.    |\n",
    "| 1st_Road_Class                               | N/A                                          | \n",
    "| 1st_Road_Number                              | N/A                                          | \n",
    "| 2nd_Road_Class                               | N/A                                          | \n",
    "| 2nd_Road_Number                              | N/A                                          | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['Unnamed: 0','Did_Police_Officer_Attend_Scene_of_Accident',\n",
    "             'Local_Authority_(Highway)','Location_Easting_OSGR',\n",
    "             'Location_Northing_OSGR','LSOA_of_Accident_Location','Police_Force',\n",
    "             'InScotland','Vehicle_Reference','model','Year_x','Year_y',\n",
    "             '1st_Road_Number','1st_Road_Class','2nd_Road_Number','2nd_Road_Class'\n",
    "            ]\n",
    "df.drop(drop_cols,axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping Out Accidents in the UK\n",
    "\n",
    "The dataset includes a `.geojson` file, which we can use to generate a map of accidents using the logitude and latitude coordinates for each accident.  First lets compile the districts and the number of accidents in each district into a separate DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of accidents by district and put them into a dataframe\n",
    "uk_dist = df['Local_Authority_(District)'].value_counts().reset_index()\n",
    "uk_dist.rename(columns={'index':'district','Local_Authority_(District)':'count'}, inplace=True)\n",
    "uk_dist#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_dist.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average coordinates to center the map on\n",
    "coords = [df['Latitude'].mean(),df['Longitude'].mean()]\n",
    "f = folium.Figure(width=750, height=750)\n",
    "uk_map = folium.Map(location=coords, zoom_start=7, min_zoom = 2).add_to(f)\n",
    "uk_map\n",
    "# uk_map.save('/Users/rem/IBM_data_science/9_Capstone/figures/uk_map.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average coordinates to center the map on\n",
    "coords = [df['Latitude'].mean(),df['Longitude'].mean()]\n",
    "f = folium.Figure(width=750, height=750)\n",
    "uk_map = folium.Map(location=coords, zoom_start=7, min_zoom = 2).add_to(f)\n",
    "\n",
    "# Load geojson file\n",
    "uk_geo = '../data/Local_Authority_Districts__December_2017__Boundaries_in_the_UK__WGS84_.geojson'\n",
    "\n",
    "# create a numpy array of length 6 and has linear spacing from the minium total immigration to the maximum total immigration\n",
    "threshold_scale = np.linspace(uk_dist['count'].min(),\n",
    "                              uk_dist['count'].max(),\n",
    "                              6, dtype=int)\n",
    "print(threshold_scale)\n",
    "threshold_scale = threshold_scale.tolist() # change the numpy array to a list\n",
    "threshold_scale[-1] = threshold_scale[-1] + 1 # make sure that the last value of the list is greater than the maximum immigration\n",
    "\n",
    "# Create a choropleth map\n",
    "uk_map.choropleth(\n",
    "    geo_data=uk_geo,\n",
    "    data=uk_dist,\n",
    "    columns=['district', 'count'],\n",
    "    key_on='feature.properties.lad17nm',\n",
    "    threshold_scale=threshold_scale,\n",
    "    fill_color='YlOrRd', \n",
    "    fill_opacity=0.7, \n",
    "    line_opacity=0.2,\n",
    "    legend_name='Frequency of Accidents in UK',\n",
    "    reset=True\n",
    ")\n",
    "# uk_map.save('/Users/rem/IBM_data_science/9_Capstone/figures/all_accidents.html')\n",
    "\n",
    "# Display map\n",
    "uk_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about counts as a function of accident severity?  Where are the most and least severe accidents occurring?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group the mean accident severity with district\n",
    "df_group = df.groupby(['Accident_Severity', 'Local_Authority_(District)']).size().reset_index(name='count')\n",
    "df_group.rename(columns={'Local_Authority_(District)':'district','Accident_Severity':'accident_severity'}, inplace=True)\n",
    "df_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df_group.pivot(index='district',columns='accident_severity').reset_index()\n",
    "df_pivot = pd.DataFrame(df_pivot.values,columns=['district','fatal','serious','slight'])\n",
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = folium.Figure(width=750, height=750)\n",
    "severity_map = folium.Map(location=coords, zoom_start=7).add_to(f)\n",
    "\n",
    "# create a numpy array of length 6 and has linear spacing from the minium total immigration to the maximum total immigration\n",
    "threshold_scale = np.linspace(df_pivot['fatal'].min(),\n",
    "                              df_pivot['fatal'].max(),\n",
    "                              6, dtype=float)\n",
    "threshold_scale = threshold_scale.tolist() # change the numpy array to a list\n",
    "threshold_scale[-1] = threshold_scale[-1] + 1 # make sure that the last value of the list is greater than the maximum immigration\n",
    "\n",
    "# Create a choropleth map\n",
    "severity_map.choropleth(\n",
    "    geo_data=uk_geo,\n",
    "    data=df_pivot,\n",
    "    columns=['district','fatal'],\n",
    "    key_on='feature.properties.lad17nm',\n",
    "    threshold_scale=threshold_scale,\n",
    "    fill_color='YlOrRd', \n",
    "    fill_opacity=0.7, \n",
    "    line_opacity=0.2,\n",
    "    legend_name='Frequency of Fatalities in UK',\n",
    "    reset=True\n",
    ")\n",
    "severity_map.save('/Users/rem/IBM_data_science/9_Capstone/figures/fatal_map.html')\n",
    "\n",
    "# Display map\n",
    "severity_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = folium.Figure(width=750, height=750)\n",
    "severity_map = folium.Map(location=coords, zoom_start=7).add_to(f)\n",
    "\n",
    "# create a numpy array of length 6 and has linear spacing from the minium total immigration to the maximum total immigration\n",
    "threshold_scale = np.linspace(df_pivot['serious'].min(),\n",
    "                              df_pivot['serious'].max(),\n",
    "                              6, dtype=float)\n",
    "threshold_scale = threshold_scale.tolist() # change the numpy array to a list\n",
    "threshold_scale[-1] = threshold_scale[-1] + 1 # make sure that the last value of the list is greater than the maximum immigration\n",
    "\n",
    "# Create a choropleth map\n",
    "severity_map.choropleth(\n",
    "    geo_data=uk_geo,\n",
    "    data=df_pivot,\n",
    "    columns=['district','serious'],\n",
    "    key_on='feature.properties.lad17nm',\n",
    "    threshold_scale=threshold_scale,\n",
    "    fill_color='YlOrRd', \n",
    "    fill_opacity=0.7, \n",
    "    line_opacity=0.2,\n",
    "    legend_name='Frequency of Serious Injuries in UK',\n",
    "    reset=True\n",
    ")\n",
    "severity_map.save('/Users/rem/IBM_data_science/9_Capstone/figures/serious_map.html')\n",
    "\n",
    "# Display map\n",
    "severity_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = folium.Figure(width=750, height=750)\n",
    "severity_map = folium.Map(location=coords, zoom_start=7).add_to(f)\n",
    "\n",
    "# create a numpy array of length 6 and has linear spacing from the minium total immigration to the maximum total immigration\n",
    "threshold_scale = np.linspace(df_pivot['slight'].min(),\n",
    "                              df_pivot['slight'].max(),\n",
    "                              6, dtype=float)\n",
    "threshold_scale = threshold_scale.tolist() # change the numpy array to a list\n",
    "threshold_scale[-1] = threshold_scale[-1] + 1 # make sure that the last value of the list is greater than the maximum immigration\n",
    "\n",
    "# Create a choropleth map\n",
    "severity_map.choropleth(\n",
    "    geo_data=uk_geo,\n",
    "    data=df_pivot,\n",
    "    columns=['district','slight'],\n",
    "    key_on='feature.properties.lad17nm',\n",
    "    threshold_scale=threshold_scale,\n",
    "    fill_color='YlOrRd', \n",
    "    fill_opacity=0.7, \n",
    "    line_opacity=0.2,\n",
    "    legend_name='Frequency of Slight Injuries in UK',\n",
    "    reset=True\n",
    ")\n",
    "severity_map.save('/Users/rem/IBM_data_science/9_Capstone/figures/slight_map.html')\n",
    "# Display map\n",
    "severity_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from these maps that higher accident severities and occurrences are associated with urban centers, and lower accident severities and occurrences with more suburban or rural districts.  Surprisingly, London has relatively few accidents, and lower severity compared to say Birmingham. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timeline of Accidents\n",
    "\n",
    "We can also visualize accidents as a function of time.  Let's get the number of accidents per unique date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of accidents by district and put them into a dataframe\n",
    "df_date = df['Date'].value_counts().reset_index()\n",
    "df_date.rename(columns={'index':'date','Date':'count'}, inplace=True)\n",
    "df_date.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates to datetime objects\n",
    "df_date['date'] = pd.to_datetime(df_date['date'])\n",
    "df_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,4))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "sns.lineplot(ax=ax1,data=df_date, x='date', y='count',\n",
    "            linewidth=0.5)\n",
    "\n",
    "fontsize=14\n",
    "ax1.set_title('Timeline of Accidents in the UK (2005-2017)',fontsize=fontsize)\n",
    "ax1.set_ylabel('Number of Accidents',fontsize=fontsize)\n",
    "ax1.set_xlabel('Date',fontsize=fontsize)\n",
    "ax1.set_xticks(['2005-01-01','2006-01-01','2007-01-01','2008-01-01','2009-01-01','2010-01-01',\n",
    "                '2011-01-01','2012-01-01','2013-01-01','2014-01-01','2015-01-01','2016-01-01',\n",
    "                '2017-01-01'])\n",
    "ax1.set_xticklabels(['2005','2006','2007','2008','2009','2010',\n",
    "                '2011','2012','2013','2014','2015','2016',\n",
    "                '2017'])\n",
    "ax1.tick_params(axis='both', labelsize=fontsize)\n",
    "ax1.set_xlim('2005','2017')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('/Users/rem/IBM_data_science/9_Capstone/figures/accidents_timeline.png',dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would seem that accidents are increasing over time, however the cause of this is likely outside the scope of this data.  Over this long period of time, there could be increased reporting due to better data collection, or simply an increase of drivers on the road. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeline by Accident Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the mean accident severity with date\n",
    "df_group = df.groupby(['Accident_Severity', 'Date']).size().reset_index(name='count')\n",
    "df_group.rename(columns={'Date':'date','Accident_Severity':'accident_severity'}, inplace=True)\n",
    "# df_group['date'] = pd.to_datetime(df_date['date'])\n",
    "df_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df_group.pivot(index='date',columns='accident_severity').reset_index()\n",
    "df_pivot = pd.DataFrame(df_pivot.values,columns=['date','fatal','serious','slight'])\n",
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot['date'] = pd.to_datetime(df_pivot['date'])\n",
    "df_pivot['fatal'].replace(np.nan,0.0,inplace=True)\n",
    "# df_pivot['fatal'] = df_pivot['fatal']/df_pivot['fatal'].sum()\n",
    "df_pivot['serious'].replace(np.nan,0.0,inplace=True)\n",
    "# df_pivot['serious'] = df_pivot['serious']/df_pivot['serious'].sum()\n",
    "\n",
    "df_pivot['slight'].replace(np.nan,0.0,inplace=True)\n",
    "# df_pivot['slight'] = df_pivot['slight']/df_pivot['slight'].sum()\n",
    "\n",
    "df_pivot['slight_%'] = df_pivot['slight']/df_pivot[['fatal','serious','slight']].sum(axis=1)*100.0\n",
    "df_pivot['serious_%'] = df_pivot['serious']/df_pivot[['fatal','serious','slight']].sum(axis=1)*100.0\n",
    "df_pivot['fatal_%'] = df_pivot['fatal']/df_pivot[['fatal','serious','slight']].sum(axis=1)*100.0\n",
    "\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,4))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "sns.lineplot(ax=ax1,data=df_pivot, x='date', y='fatal_%',\n",
    "            linewidth=0.5, color='xkcd:dark red', label='Fatal')\n",
    "sns.lineplot(ax=ax1,data=df_pivot, x='date', y='serious_%',\n",
    "            linewidth=0.5, color='xkcd:dark orange', label='Serious')\n",
    "sns.lineplot(ax=ax1,data=df_pivot, x='date', y='slight_%',\n",
    "            linewidth=0.5, color='xkcd:goldenrod', label='Slight')\n",
    "\n",
    "fontsize=14\n",
    "ax1.set_title('Timeline of Percent of Accidents by Date (2005-2017)',fontsize=fontsize)\n",
    "ax1.set_ylabel('Percent of Accidents',fontsize=fontsize)\n",
    "ax1.set_xlabel('Date',fontsize=fontsize)\n",
    "ax1.set_xticks(['2005-01-01','2006-01-01','2007-01-01','2008-01-01','2009-01-01','2010-01-01',\n",
    "                '2011-01-01','2012-01-01','2013-01-01','2014-01-01','2015-01-01','2016-01-01',\n",
    "                '2017-01-01'])\n",
    "ax1.set_xticklabels(['2005','2006','2007','2008','2009','2010',\n",
    "                '2011','2012','2013','2014','2015','2016',\n",
    "                '2017'])\n",
    "ax1.tick_params(axis='both', labelsize=fontsize)\n",
    "ax1.set_xlim('2005','2017')\n",
    "ax1.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('/Users/rem/IBM_data_science/9_Capstone/figures/severity_timeline.png',dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Variable Feature Selection\n",
    "\n",
    "Now we explore the data to determine which *continuous* features would be good predictors of accident severity.  There are only 5 continuous variables whose descriptions would be useful in the context of accident prevention.  In the below table, we list the continuous variables under consideration and their brief descriptions:\n",
    "\n",
    "| Attribute                                    | \n",
    "| -------------------------------------------- | \n",
    "| Time | \n",
    "| Age_of_Vehicle | \n",
    "| Engine_Capacity_.CC. | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we can do is create a correlation scatter matrix to visualize any linear relationships among these variables with accident severity, to see which of these are good predictors.  To do this, we first need to convert `Accident_Severity` from a categorical variable to a numerical one by label encoding (*note*: when we perform ML analysis, we must use one-hot encoding, not label encoding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract continuous features\n",
    "cont_attrib = ['Time','Age_of_Vehicle','Engine_Capacity_.CC.','Accident_Severity']\n",
    "df_cont = df.copy()[cont_attrib]\n",
    "df_cont.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets also conver time to a datatime format\n",
    "hour = pd.to_datetime(df_cont['Time'],format= '%H:%M').dt.hour\n",
    "minute = pd.to_datetime(df_cont['Time'],format= '%H:%M').dt.minute/60.0\n",
    "df_cont['time'] = hour+minute\n",
    "df_cont.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,4))\n",
    "ax1 = fig.add_subplot(1,3,1)\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "\n",
    "sns.boxplot(ax=ax1, x=\"Accident_Severity\", y=\"Age_of_Vehicle\", data=df_cont)\n",
    "sns.boxplot(ax=ax2, x=\"Accident_Severity\", y=\"Engine_Capacity_.CC.\", data=df_cont)\n",
    "sns.boxplot(ax=ax3, x=\"Accident_Severity\", y=\"time\", data=df_cont)\n",
    "\n",
    "fontsize=14\n",
    "# ax1.set_title('',fontsize=fontsize)\n",
    "ax1.set_xlabel('Accident Severity',fontsize=fontsize)\n",
    "ax2.set_xlabel('Accident Severity',fontsize=fontsize)\n",
    "ax3.set_xlabel('Accident Severity',fontsize=fontsize)\n",
    "\n",
    "ax1.set_ylabel('Age of Vehicle',fontsize=fontsize)\n",
    "ax2.set_ylabel('Engine Capacity (CC)',fontsize=fontsize)\n",
    "ax3.set_ylabel('Time of Day (24h)',fontsize=fontsize)\n",
    "ax3.set_yticks([0,6,12,18,24])\n",
    "ax3.set_yticklabels(['00:00','06:00','12:00','18:00','24:00'])\n",
    "\n",
    "ax1.tick_params(axis='both', labelsize=fontsize)\n",
    "ax2.tick_params(axis='both', labelsize=fontsize)\n",
    "ax3.tick_params(axis='both', labelsize=fontsize)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('/Users/rem/IBM_data_science/9_Capstone/figures/continuous_attrib.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are significant outliers for the age and engine capacity as a function of accident severity, which means these are probably not good predictors.  However, there are no outliers for the time predictor, which indicates that there is at least a certain time of day when accident are more probable regardless of severity.  This means it isn't a good predictor of accident severity, but moreso a predictor of when accidents are more likely to occur.  We thus do not include any of these variables in our model for predicting accident severity.\n",
    "<br><br/>\n",
    "We can see this better by plotting each distribution of accident severity as a function of time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time_slight  = df_cont.loc[df_cont['Accident_Severity']=='Slight','time']\n",
    "time_serious = df_cont.loc[df_cont['Accident_Severity']=='Serious','time']\n",
    "time_fatal   = df_cont.loc[df_cont['Accident_Severity']=='Fatal','time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "sns.kdeplot(data=time_slight,ax=ax1,\n",
    "            fill=True,palette='crest',alpha=0.25,\n",
    "            label='slight',linewidth=0)\n",
    "sns.kdeplot(data=time_serious,ax=ax1,\n",
    "           fill=True,palette='crest',alpha=0.25,\n",
    "           label='serious',linewidth=0)\n",
    "sns.kdeplot(data=time_fatal,ax=ax1,\n",
    "           fill=True,palette='crest',alpha=0.25,\n",
    "           label='fatal',linewidth=0)\n",
    "fontsize=14\n",
    "ax1.set_ylim(0,0.11)\n",
    "ax1.set_xlabel('Time',fontsize=fontsize)\n",
    "ax1.set_ylabel('Accidents (Normalized Density)',fontsize=fontsize)\n",
    "ax1.set_title('KDE of Accidents as a function of Time of Day',fontsize=fontsize)\n",
    "ax1.set_xticks([0,2,4,6,8,10,12,14,16,18,20,22,24])\n",
    "ax1.set_xticklabels(['12:00 AM','2:00 AM','4:00 AM','6:00 AM','8:00 AM','10:00 AM',\n",
    "                     '12:00 PM','2:00 PM','4:00 PM','6:00 PM','8:00 PM','10:00 PM','12:00 AM'\n",
    "                    ], rotation=45, horizontalalignment='right')\n",
    "\n",
    "ax1.annotate('Morning Rush', xy=(0.39, 0.77),  xycoords='axes fraction',\n",
    "            xytext=(0.39, 0.77), textcoords='axes fraction',\n",
    "            horizontalalignment='center', verticalalignment='center',\n",
    "            fontsize=fontsize)\n",
    "ax1.annotate('Evening Rush', xy=(0.67, 0.90),  xycoords='axes fraction',\n",
    "            xytext=(0.67, 0.90), textcoords='axes fraction',\n",
    "            horizontalalignment='center', verticalalignment='center',\n",
    "            fontsize=fontsize)\n",
    "ax1.tick_params(axis='both', labelsize=fontsize)\n",
    "\n",
    "ax1.legend(fontsize=fontsize)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('/Users/rem/IBM_data_science/9_Capstone/figures/accidents_time_of_day.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day of Week\n",
    "\n",
    "We could also see if the day of the week is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group the mean accident severity with district\n",
    "df_group = df.groupby(['Accident_Severity', 'Day_of_Week']).size().reset_index(name='count')\n",
    "df_group.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df_group.pivot(index='Day_of_Week',columns='Accident_Severity').reset_index()\n",
    "df_pivot = pd.DataFrame(df_pivot.values,columns=['Day_of_Week','fatal','serious','slight'])\n",
    "df_pivot.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each day so it is a percentage\n",
    "df_pivot['fatal'] = df_pivot['fatal']/df_pivot['fatal'].sum()*100.\n",
    "df_pivot['serious'] = df_pivot['serious']/df_pivot['serious'].sum()*100.\n",
    "df_pivot['slight'] = df_pivot['slight']/df_pivot['slight'].sum()*100.\n",
    "# Set index to day of week\n",
    "df_pivot = df_pivot.set_index(['Day_of_Week'])\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "categories = ['Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday']\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "df_pivot.reindex(categories).plot(kind='bar', stacked=True, figsize=(10, 5),\n",
    "                            fontsize=12, rot=0,ax=ax1,\n",
    "                                  colormap=ListedColormap(sns.color_palette(\"YlOrRd_r\", 10)))\n",
    "\n",
    "fontsize=14\n",
    "ax1.set_title('Percentage of Accidents Per Day of Week',fontsize=fontsize)\n",
    "ax1.set_ylabel('\\% of Accidents',fontsize=fontsize)\n",
    "ax1.set_xlabel('Day of Week',fontsize=fontsize)\n",
    "ax1.set_xticks(range(len(categories)))\n",
    "ax1.set_xticklabels(categories)\n",
    "ax1.tick_params(axis='both', labelsize=fontsize)\n",
    "ax1.legend(fontsize=fontsize-2)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('/Users/rem/IBM_data_science/9_Capstone/figures/accidents_day_of_week.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that day of week also does not have a significant influence on accident severity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Variables \n",
    "\n",
    "Most of our predictors are categorical.  Below is a list of the predictors we will investigate:\n",
    "\n",
    "| Attribute                                    | \n",
    "| -------------------------------------------- | \n",
    "| Light_Conditions |\n",
    "| Road_Surface_Conditions | \n",
    "| Road_Type | \n",
    "| Speed_limit | \n",
    "| Urban_or_Rural_Area |\n",
    "| Weather_Conditions | \n",
    "| Age_Band_of_Driver | \n",
    "| Vehicle_Type |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the categories in each column \n",
    "columns = ['Light_Conditions','Road_Surface_Conditions','Road_Type','Speed_limit',\n",
    "           'Urban_or_Rural_Area','Weather_Conditions','Age_Band_of_Driver','Vehicle_Type']\n",
    "\n",
    "for c in columns:\n",
    "    print('Column: %s' % c)\n",
    "    print('-------------------------------------------------')\n",
    "    print(df[c].value_counts())\n",
    "    print('-------------------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's make a copy of the dataframe that we can manipulate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df.copy()[['Accident_Severity','Light_Conditions','Road_Surface_Conditions','Road_Type','Speed_limit',\n",
    "           'Urban_or_Rural_Area','Weather_Conditions','Age_Band_of_Driver','Vehicle_Type']]\n",
    "print(df_cat.shape)\n",
    "df_cat.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove missing Data\n",
    "\n",
    "Fortunately, because of the large size of this dataset, we can afford to remove data for which there is no missing value.  In reality, trying to replace these categorical data with something else would probably cause more problems than solve, so we just remove missing or unknown data altogeher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing/unknown data from Light_Conditions\n",
    "df_cat.drop(df_cat[df_cat['Light_Conditions']=='Data missing or out of range'].index, inplace = True)\n",
    "# Also remove unknown lighting conditions\n",
    "df_cat.drop(df_cat[df_cat['Light_Conditions']=='Darkness - lighting unknown'].index, inplace = True) \n",
    "# Remove missing/unknown data from Road_Surface_Conditions\n",
    "df_cat.drop(df_cat[df_cat['Road_Surface_Conditions']=='Data missing or out of range'].index, inplace = True) \n",
    "# Remove missing/unknown data from Road_Type\n",
    "df_cat.drop(df_cat[df_cat['Road_Type']=='Data missing or out of range'].index, inplace = True) \n",
    "df_cat.drop(df_cat[df_cat['Road_Type']=='Unknown'].index, inplace = True) \n",
    "# Remove missing/unknown data from Urban_or_Rural_Area\n",
    "df_cat.drop(df_cat[df_cat['Urban_or_Rural_Area']=='Unallocated'].index, inplace = True) \n",
    "# Remove missing/unknown data from Weather_Conditions\n",
    "df_cat.drop(df_cat[df_cat['Weather_Conditions']=='Data missing or out of range'].index, inplace = True) \n",
    "df_cat.drop(df_cat[df_cat['Weather_Conditions']=='Unknown'].index, inplace = True) \n",
    "df_cat.drop(df_cat[df_cat['Weather_Conditions']=='Other'].index, inplace = True) \n",
    "# Remove missing/unknown data from Weather_Conditions\n",
    "df_cat.drop(df_cat[df_cat['Age_Band_of_Driver']=='Data missing or out of range'].index, inplace = True) \n",
    "# Let's get rid of some non-sensical driver ages\n",
    "# The legal driving age in the UK is 15 years, 9 months or ~16 years; any younger and its illegal anyway.  These illegal age bands \n",
    "# just add noise the the data, and aren't worth keeping.\n",
    "df_cat.drop(df_cat[df_cat['Age_Band_of_Driver']=='11 - 15'].index, inplace = True) \n",
    "df_cat.drop(df_cat[df_cat['Age_Band_of_Driver']=='6 - 10'].index, inplace = True) \n",
    "df_cat.drop(df_cat[df_cat['Age_Band_of_Driver']=='0 - 5'].index, inplace = True) \n",
    "# Remove missing/unknown data, and uncommon means of transportation from Vehicle_Type\n",
    "df_cat.drop(df_cat[df_cat['Vehicle_Type']=='Data missing or out of range'].index, inplace = True) \n",
    "df_cat.drop(df_cat[df_cat['Vehicle_Type']=='Other vehicle'].index, inplace = True) \n",
    "df_cat.drop(df_cat[df_cat['Vehicle_Type']=='Ridden horse'].index, inplace = True) \n",
    "df_cat.drop(df_cat[df_cat['Vehicle_Type']=='Tram'].index, inplace = True) \n",
    "\n",
    "print(df_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accident Severity & Light Conditions\n",
    "\n",
    "We can use a heatmap to visualize the relationship between these two categorical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['Light_Conditions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"Darkness - lights unlit\" may as well be \"Darkness - no lighting\", so we combine these two categories\n",
    "df_cat['Light_Conditions'].replace('Darkness - lights unlit','Darkness - no lighting',inplace=True)\n",
    "df_cat['Light_Conditions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df_cat.groupby(['Accident_Severity', 'Light_Conditions']).size().reset_index(name='count')\n",
    "\n",
    "df_pivot = pd.pivot_table(df_group, values='count', \n",
    "                     index=['Light_Conditions'], \n",
    "                     columns='Accident_Severity')\n",
    "# Normalize each day so it is a percentage\n",
    "df_pivot['Fatal'] = df_pivot['Fatal']/df_pivot['Fatal'].sum()*100.\n",
    "df_pivot['Serious'] = df_pivot['Serious']/df_pivot['Serious'].sum()*100.\n",
    "df_pivot['Slight'] = df_pivot['Slight']/df_pivot['Slight'].sum()*100.\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig  = plt.figure(figsize=(7,7))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "sns.heatmap(df_pivot, annot=True,fmt='.3f', linewidths=.3, ax=ax1,  cmap='GnBu')\n",
    "\n",
    "fontsize=14\n",
    "ax1.set_title('Percentage of Accidents by Accident Severity: Light Conditions',fontsize=fontsize-2)\n",
    "ax1.set_ylabel('Light Conditions',fontsize=fontsize)\n",
    "ax1.set_xlabel('Accident Severity',fontsize=fontsize)\n",
    "# ax1.set_xticks(range(len(categories)))\n",
    "# ax1.set_xticklabels(categories)\n",
    "ax1.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.yticks(va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('/Users/rem/IBM_data_science/9_Capstone/figures/light_conditions.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would seem that when there is lights lit or daylight, accident occurence is nearly the same across severities, but severity dramatically increases when there is no lighting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accident Severity & Road Surface Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['Road_Surface_Conditions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['Road_Surface_Conditions'].replace('Wet or damp','Wet',inplace=True)\n",
    "df_cat['Road_Surface_Conditions'].replace('Frost or ice','Ice',inplace=True)\n",
    "df_cat['Road_Surface_Conditions'].replace('Flood over 3cm. deep','Flood',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['Road_Surface_Conditions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df_cat.groupby(['Accident_Severity', 'Road_Surface_Conditions']).size().reset_index(name='count')\n",
    "\n",
    "df_pivot = pd.pivot_table(df_group, values='count', \n",
    "                     index=['Road_Surface_Conditions'], \n",
    "                     columns='Accident_Severity')\n",
    "# Normalize each day so it is a percentage\n",
    "df_pivot['Fatal'] = df_pivot['Fatal']/df_pivot['Fatal'].sum()*100.\n",
    "df_pivot['Serious'] = df_pivot['Serious']/df_pivot['Serious'].sum()*100.\n",
    "df_pivot['Slight'] = df_pivot['Slight']/df_pivot['Slight'].sum()*100.\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig  = plt.figure(figsize=(7,7))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "sns.heatmap(df_pivot, annot=True,fmt='.3f', linewidths=.3, ax=ax1,  cmap='YlOrRd')\n",
    "\n",
    "fontsize=14\n",
    "ax1.set_title('Percentage of Accidents by Accident Severity: Road Surface Conditions',fontsize=fontsize-2)\n",
    "ax1.set_ylabel('Road Surface Conditions',fontsize=fontsize)\n",
    "ax1.set_xlabel('Accident Severity',fontsize=fontsize)\n",
    "# ax1.set_xticks(range(len(categories)))\n",
    "# ax1.set_xticklabels(categories)\n",
    "ax1.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.yticks(va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('/Users/rem/IBM_data_science/9_Capstone/figures/road_surface_cond.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that accident severity is nearly constant across all road surface conditions, and therefore may be a bad predictor of severity.  This can be driven by factors we cannot account for with this data alone, such as the fact that drivers probably drive more cautiously during non-dry conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accident Severity & Road Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['Road_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['Road_Type'].replace('Slip road','On/off ramp',inplace=True)\n",
    "df_cat['Road_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df_cat.groupby(['Accident_Severity', 'Road_Type']).size().reset_index(name='count')\n",
    "\n",
    "df_pivot = pd.pivot_table(df_group, values='count', \n",
    "                     index=['Road_Type'], \n",
    "                     columns='Accident_Severity')\n",
    "# Normalize each day so it is a percentage\n",
    "df_pivot['Fatal'] = df_pivot['Fatal']/df_pivot['Fatal'].sum()*100.\n",
    "df_pivot['Serious'] = df_pivot['Serious']/df_pivot['Serious'].sum()*100.\n",
    "df_pivot['Slight'] = df_pivot['Slight']/df_pivot['Slight'].sum()*100.\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig  = plt.figure(figsize=(7,10))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "sns.heatmap(df_pivot, annot=True,fmt='.3f', linewidths=.3, ax=ax1,  cmap='YlGnBu')\n",
    "\n",
    "fontsize=14\n",
    "ax1.set_title('Percentage of Accidents by Accident Severity: Road Type',fontsize=fontsize-1)\n",
    "ax1.set_ylabel('Road Type',fontsize=fontsize)\n",
    "ax1.set_xlabel('Accident Severity',fontsize=fontsize)\n",
    "# ax1.set_xticks(range(len(categories)))\n",
    "# ax1.set_xticklabels(categories)\n",
    "ax1.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.yticks(va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('/Users/rem/IBM_data_science/9_Capstone/figures/road_type.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, not many good predictors fo severity here.  The only one that stands out is roundabouts, where fatal accidents decrease sharpy from slight accidents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accident Severity & Speed Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['Speed_limit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine some of these labels to just two: \"Under 40\" and \"Over 40\".  We use 40 km/s (25 mph) because this is a typical resident speed limit, whereas anything over 40 km/s is closer to highway speed limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['Speed_limit'].replace(15.0,'Under 40 km/s',inplace=True)\n",
    "df_cat['Speed_limit'].replace(10.0,'Under 40 km/s',inplace=True)\n",
    "df_cat['Speed_limit'].replace(20.0,'Under 40 km/s',inplace=True)\n",
    "df_cat['Speed_limit'].replace(30.0,'Under 40 km/s',inplace=True)\n",
    "df_cat['Speed_limit'].replace(40.0,'Under 40 km/s',inplace=True)\n",
    "df_cat['Speed_limit'].replace(50.0,'Over 40 km/s',inplace=True)\n",
    "df_cat['Speed_limit'].replace(60.0,'Over 40 km/s',inplace=True)\n",
    "df_cat['Speed_limit'].replace(70.0,'Over 40 km/s',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['Speed_limit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df_cat.groupby(['Accident_Severity', 'Speed_limit']).size().reset_index(name='count')\n",
    "\n",
    "df_pivot = df_group.pivot(index='Speed_limit',columns='Accident_Severity').reset_index()\n",
    "df_pivot = pd.DataFrame(df_pivot.values,columns=['Speed_limit','fatal','serious','slight'])\n",
    "\n",
    "# Normalize each day so it is a percentage\n",
    "df_pivot['fatal'] = df_pivot['fatal']/df_pivot['fatal'].sum()*100.\n",
    "df_pivot['serious'] = df_pivot['serious']/df_pivot['serious'].sum()*100.\n",
    "df_pivot['slight'] = df_pivot['slight']/df_pivot['slight'].sum()*100.\n",
    "# Get rid of NaN\n",
    "df_pivot.replace(np.nan, 0.0, inplace = True)\n",
    "\n",
    "# Set index to day of week\n",
    "df_pivot = df_pivot.set_index(['Speed_limit'])\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "fig  = plt.figure(figsize=(7,4))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "sns.heatmap(df_pivot, annot=True,fmt='.3f', linewidths=.3, ax=ax1,  cmap='PuBuGn')\n",
    "\n",
    "fontsize=14\n",
    "ax1.set_title('Percentage of Accidents by Accident Severity: Posted Speed Limit',fontsize=fontsize-2)\n",
    "ax1.set_ylabel('Posted Speed Limit',fontsize=fontsize)\n",
    "ax1.set_xlabel('Accident Severity',fontsize=fontsize)\n",
    "# ax1.set_xticks(range(len(categories)))\n",
    "# ax1.set_xticklabels(categories)\n",
    "ax1.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.yticks(va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('/Users/rem/IBM_data_science/9_Capstone/figures/speed_limit.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would seem that fatalities increase dramatically when travelling at 60 km/hr, compared to other speeds, and slight accidents increase dramatically at 30 km/hr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accident Severity & Urban or Rural Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['Urban_or_Rural_Area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df_cat.groupby(['Accident_Severity', 'Urban_or_Rural_Area']).size().reset_index(name='count')\n",
    "\n",
    "df_pivot = df_group.pivot(index='Urban_or_Rural_Area',columns='Accident_Severity').reset_index()\n",
    "df_pivot = pd.DataFrame(df_pivot.values,columns=['Urban_or_Rural_Area','fatal','serious','slight'])\n",
    "\n",
    "# Normalize each day so it is a percentage\n",
    "df_pivot['fatal'] = df_pivot['fatal']/df_pivot['fatal'].sum()*100.\n",
    "df_pivot['serious'] = df_pivot['serious']/df_pivot['serious'].sum()*100.\n",
    "df_pivot['slight'] = df_pivot['slight']/df_pivot['slight'].sum()*100.\n",
    "# Get rid of NaN\n",
    "df_pivot.replace(np.nan, 0.0, inplace = True)\n",
    "\n",
    "# Set index to day of week\n",
    "df_pivot = df_pivot.set_index(['Urban_or_Rural_Area'])\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "categories = ['Urban','Rural']\n",
    "\n",
    "fig  = plt.figure(figsize=(7,4))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "# df_pivot.plot(kind='bar', stacked=True, figsize=(10, 5),\n",
    "#                             fontsize=12, rot=0,\n",
    "#                                   colormap=ListedColormap(sns.color_palette(\"RdPu\", 10)))\n",
    "\n",
    "sns.heatmap(df_pivot, annot=True,fmt='.3f', linewidths=.3, ax=ax1,  cmap='BuGn')\n",
    "\n",
    "fontsize=14\n",
    "ax1.set_title('Percentage of Accidents by Accident Severity: Urban or Rural',fontsize=fontsize-2)\n",
    "ax1.set_ylabel('Urban or Rural',fontsize=fontsize)\n",
    "ax1.set_xlabel('Accident Severity',fontsize=fontsize)\n",
    "# ax1.set_xticks(range(len(categories)))\n",
    "# ax1.set_xticklabels(categories)\n",
    "ax1.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.yticks(va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('/Users/rem/IBM_data_science/9_Capstone/figures/urban_rural.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also interestingly, there are more fatalities for rural areas than uban ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accident Severity & Age Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['Age_Band_of_Driver'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine features into \"Adolescient\" (16-20), \"Adult\" (21-65), and \"Senior\" (66-75+):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['Age_Band_of_Driver'].replace('16 - 20','Adolescent',inplace=True)\n",
    "\n",
    "df_cat['Age_Band_of_Driver'].replace('21 - 25','Adult',inplace=True)\n",
    "df_cat['Age_Band_of_Driver'].replace('26 - 35','Adult',inplace=True)\n",
    "df_cat['Age_Band_of_Driver'].replace('36 - 45','Adult',inplace=True)\n",
    "df_cat['Age_Band_of_Driver'].replace('46 - 55','Adult',inplace=True)\n",
    "df_cat['Age_Band_of_Driver'].replace('56 - 65','Adult',inplace=True)\n",
    "\n",
    "df_cat['Age_Band_of_Driver'].replace('66 - 75','Senior',inplace=True)\n",
    "df_cat['Age_Band_of_Driver'].replace('Over 75','Senior',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df_cat.groupby(['Accident_Severity', 'Age_Band_of_Driver']).size().reset_index(name='count')\n",
    "\n",
    "df_pivot = pd.pivot_table(df_group, values='count', \n",
    "                     index=['Age_Band_of_Driver'], \n",
    "                     columns='Accident_Severity')\n",
    "# Normalize each day so it is a percentage\n",
    "df_pivot['Fatal'] = df_pivot['Fatal']/df_pivot['Fatal'].sum()*100.\n",
    "df_pivot['Serious'] = df_pivot['Serious']/df_pivot['Serious'].sum()*100.\n",
    "df_pivot['Slight'] = df_pivot['Slight']/df_pivot['Slight'].sum()*100.\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig  = plt.figure(figsize=(7,7))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "sns.heatmap(df_pivot, annot=True,fmt='.3f', linewidths=.3, ax=ax1,  cmap='BuGn')\n",
    "\n",
    "fontsize=14\n",
    "ax1.set_title('Percentage of Accidents by Accident Severity: Age Band of Driver',fontsize=fontsize-2)\n",
    "ax1.set_ylabel('Age Band of Driver',fontsize=fontsize)\n",
    "ax1.set_xlabel('Accident Severity',fontsize=fontsize)\n",
    "# ax1.set_xticks(range(len(categories)))\n",
    "# ax1.set_xticklabels(categories)\n",
    "ax1.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.yticks(va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('/Users/rem/IBM_data_science/9_Capstone/figures/age_band.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no significant trends in Age Band with accident severity.  There is a slight increase for falat versus slight accidents for drivers over 75, but this is about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accident Severity & Weather Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['Weather_Conditions'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine some of these features to make things simpler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['Weather_Conditions'].replace('Fine no high winds','Fine',inplace=True)\n",
    "df_cat['Weather_Conditions'].replace('Fine + high winds','Fine',inplace=True)\n",
    "df_cat['Weather_Conditions'].replace('Raining no high winds','Rain',inplace=True)\n",
    "df_cat['Weather_Conditions'].replace('Raining + high winds','Rain',inplace=True)\n",
    "df_cat['Weather_Conditions'].replace('Snowing no high winds','Snow',inplace=True)\n",
    "df_cat['Weather_Conditions'].replace('Snowing + high winds','Snow',inplace=True)\n",
    "df_cat['Weather_Conditions'].replace('Fog or mist','Fog',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['Weather_Conditions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df_cat.groupby(['Accident_Severity', 'Weather_Conditions']).size().reset_index(name='count')\n",
    "\n",
    "df_pivot = pd.pivot_table(df_group, values='count', \n",
    "                     index=['Weather_Conditions'], \n",
    "                     columns='Accident_Severity')\n",
    "# Normalize each day so it is a percentage\n",
    "df_pivot['Fatal'] = df_pivot['Fatal']/df_pivot['Fatal'].sum()*100.\n",
    "df_pivot['Serious'] = df_pivot['Serious']/df_pivot['Serious'].sum()*100.\n",
    "df_pivot['Slight'] = df_pivot['Slight']/df_pivot['Slight'].sum()*100.\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig  = plt.figure(figsize=(7,7))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "sns.heatmap(df_pivot, annot=True,fmt='.3f', linewidths=.3, ax=ax1,  cmap='coolwarm')\n",
    "\n",
    "fontsize=14\n",
    "ax1.set_title('Percentage of Accidents by Accident Severity: Weather Conditions',fontsize=fontsize-2)\n",
    "ax1.set_ylabel('Weather Conditions',fontsize=fontsize)\n",
    "ax1.set_xlabel('Accident Severity',fontsize=fontsize)\n",
    "# ax1.set_xticks(range(len(categories)))\n",
    "# ax1.set_xticklabels(categories)\n",
    "ax1.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.yticks(va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('/Users/rem/IBM_data_science/9_Capstone/figures/weather_cond.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, no significant trends with accident severity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accident Severity & Vehicle Type\n",
    "\n",
    "Before we start feature engineering, let's see if we can visualize how vehicle type relates to accident severity using a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_cat['Vehicle_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here is that there are a lot of categories that could be put together, such as the motorcycles, buses, and goods vehicles into single labels. \n",
    "<br><br/>\n",
    "Let's aggregate similar labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lump motorcycles into same label\n",
    "df_cat['Vehicle_Type'].replace('Electric motorcycle','Motorcycle',inplace=True)\n",
    "df_cat['Vehicle_Type'].replace('Motorcycle - unknown cc','Motorcycle',inplace=True)\n",
    "df_cat['Vehicle_Type'].replace('Motorcycle 125cc and under','Motorcycle',inplace=True)\n",
    "df_cat['Vehicle_Type'].replace('Motorcycle 50cc and under','Motorcycle',inplace=True)\n",
    "df_cat['Vehicle_Type'].replace('Motorcycle over 125cc and up to 500cc','Motorcycle',inplace=True)\n",
    "df_cat['Vehicle_Type'].replace('Motorcycle over 500cc','Motorcycle',inplace=True)\n",
    "# Buses \n",
    "df_cat['Vehicle_Type'].replace('Minibus (8 - 16 passenger seats)','Bus',inplace=True)\n",
    "df_cat['Vehicle_Type'].replace('Bus or coach (17 or more pass seats)','Bus',inplace=True)\n",
    "# Goods transportation\n",
    "df_cat['Vehicle_Type'].replace('Goods 7.5 tonnes mgw and over','Goods',inplace=True)\n",
    "df_cat['Vehicle_Type'].replace('Goods over 3.5t. and under 7.5t','Goods',inplace=True)\n",
    "df_cat['Vehicle_Type'].replace('Goods vehicle - unknown weight','Goods',inplace=True)\n",
    "df_cat['Vehicle_Type'].replace('Van / Goods 3.5 tonnes mgw or under','Goods',inplace=True)\n",
    "# Lets rename some of these labels to something simpler\n",
    "df_cat['Vehicle_Type'].replace('Taxi/Private hire car','Taxi',inplace=True)\n",
    "df_cat['Vehicle_Type'].replace('Pedal cycle','Bike',inplace=True)\n",
    "df_cat['Vehicle_Type'].replace('Agricultural vehicle','Agriculture',inplace=True)\n",
    "df_cat['Vehicle_Type'].replace('Mobility scooter','Handicap scooter',inplace=True)\n",
    "# df_cat['Vehicle_Type'].replace('Ridden horse','Horse',inplace=True)\n",
    "# df_cat['Vehicle_Type'].replace('Tram','Rail',inplace=True)\n",
    "# Let's finally remove some less common means of transportation that we have no interest in\n",
    "df_cat.drop(df_cat[df_cat['Vehicle_Type']=='Handicap scooter'].index, inplace = True) \n",
    "df_cat.drop(df_cat[df_cat['Vehicle_Type']=='Agriculture'].index, inplace = True) \n",
    "df_cat.drop(df_cat[df_cat['Vehicle_Type']=='Bus'].index, inplace = True) \n",
    "df_cat.drop(df_cat[df_cat['Vehicle_Type']=='Taxi'].index, inplace = True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the results as a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['Vehicle_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df_cat.groupby(['Accident_Severity', 'Vehicle_Type']).size().reset_index(name='count')\n",
    "\n",
    "df_pivot = pd.pivot_table(df_group, values='count', \n",
    "                     index=['Vehicle_Type'], \n",
    "                     columns='Accident_Severity')\n",
    "df_pivot['Fatal'] = df_pivot['Fatal']/df_pivot['Fatal'].sum()*100.\n",
    "df_pivot['Serious'] = df_pivot['Serious']/df_pivot['Serious'].sum()*100.\n",
    "df_pivot['Slight'] = df_pivot['Slight']/df_pivot['Slight'].sum()*100.\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig  = plt.figure(figsize=(10,10))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "sns.heatmap(df_pivot, annot=True,fmt='.3f', linewidths=.3, ax=ax1,  cmap='RdPu')\n",
    "\n",
    "fontsize=14\n",
    "ax1.set_title('Vehicle Type \\& Accident Severity',fontsize=fontsize)\n",
    "ax1.set_ylabel('\\% of Accidents',fontsize=fontsize)\n",
    "ax1.set_xlabel('Accident Severity',fontsize=fontsize)\n",
    "# ax1.set_xticks(range(len(categories)))\n",
    "# ax1.set_xticklabels(categories)\n",
    "ax1.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.yticks(va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('/Users/rem/IBM_data_science/9_Capstone/figures/vehicle_type.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here we can see that Vehicle_Type is a good indicator for accident severity for at least Good and Motorcycle vehicle types, indicated by clear differences between severities.  Cars still dominate the fraction of accidents in each severity category, but 15% more for slight severities than fatal severities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the Final DataFrame as a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df_cat.columns:\n",
    "    print('Column: %s' % c)\n",
    "    print('-------------------------------------------------')\n",
    "    print(df_cat[c].value_counts())\n",
    "    print('-------------------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cat.to_csv('uk_accident_data.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
